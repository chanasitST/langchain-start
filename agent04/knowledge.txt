LangChain is a framework for developing applications powered by language models.
    It enables applications that:
    - Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)
    - Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)
    
    The main value props of LangChain are:
    1. Components: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not.
    2. Off-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks.
    
    LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. 
    Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence.
    LangGraph allows you to define flows that involve cycles, essential for most agentic architectures.
    It provides fine-grained control over both the flow and state of your application, crucial for creating reliable agents.
    
    Semantic Chunking is a technique to split text based on meaning. 
    Instead of splitting by a fixed number of characters, it uses embeddings to find "breakpoints" where the topic changes.
    This results in chunks that are semantically complete, improving retrieval quality.
    
    RAG (Retrieval Augmented Generation) is a technique that combines retrieval and generation.
    First, we retrieve relevant documents from a knowledge base using the user's query.
    Then, we pass those documents as context to an LLM, which generates a response based on that context.
    This allows the LLM to provide accurate, up-to-date information without hallucinating.